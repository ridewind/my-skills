---
name: code-review-orchestrator
description: This skill should be used when the user asks to "review code", "do a code review", "review my branch", "review MR !1234", "review PR #567", "review feature/auth branch", "review feature/auth vs dev", "check code quality", "review entire project", "review all code", or wants to orchestrate multiple code review skills/subagents. Coordinates parallel code reviews using multiple review skills and generates comprehensive summary reports.
version: 0.5.0
---

# Code Review Orchestrator

Orchestrate comprehensive code reviews by coordinating multiple review skills and subagents in parallel, then consolidate findings into actionable reports.

## Purpose

This skill manages the complete code review workflow:
- Collect and organize code content for review (diffs, commits, branches, MR/PR info)
- Coordinate multiple subagents using different review skills
- Consolidate individual review reports into comprehensive summaries
- Help users identify and fix issues

## Debug Mode

**Configuration**: Debug mode can be controlled via user input or interactive confirmation.

**How to Enable Debug Mode**:
1. **Automatic**: Include debug keywords in your request (`debug`, `verbose`, `è°ƒè¯•`, `è¯¦ç»†`, `--debug`, `-v`)
2. **Interactive**: When prompted, select "å¯ç”¨è°ƒè¯•" to enable detailed logging

**Debug Mode Records** (when enabled):
- All checkpoints and decision points
- User selections and confirmations
- Subagent launch and completion status
- Timestamps and progress tracking
- Complete interaction history

**Output**: Saved to `DEBUG-SESSION.md` in the working directory

**To force always-on**: Change Step 0 to always set `debug_mode = True`

## When to Use

Trigger this skill when users request code review with phrases like:
- "Review my code"
- "Review feature/auth branch"
- "Review MR !1234" / "Review PR #567"
- "Review feature/auth vs dev branch"
- "Do a comprehensive code review"

## Workflow

**DEBUG MODE CONFIGURATION**: Debug mode can be controlled via user input or interactive confirmation.

### Step 0: Debug Mode Detection (Optional)

**ğŸ” DEBUG [Step 0/7]**: Checking if debug mode should be enabled

Determine whether to enable detailed debug logging based on user input or confirmation.

**Automatic Detection**:
Check user's initial input for debug-related keywords:
- `debug`, `verbose`, `detail`, `log`, `trace`
- `--debug`, `-v`, `--verbose`
- Chinese: `è°ƒè¯•`, `è¯¦ç»†`, `æ—¥å¿—`, `è®°å½•`

**Detection Logic**:
```python
# Get user's initial input
user_input = "<user's original request>"  # e.g., "Review my code with debug"
user_input_lower = user_input.lower()

# Keywords that trigger debug mode
debug_keywords = [
    'debug', 'verbose', 'detail', 'log', 'trace',
    '--debug', '-v', '--verbose',
    'è°ƒè¯•', 'è¯¦ç»†', 'æ—¥å¿—', 'è®°å½•'
]

# Auto-detect
debug_mode = any(keyword in user_input_lower for keyword in debug_keywords)

if debug_mode:
    print("âœ… Debug mode automatically enabled (keyword detected)")
else:
    # Ask user if they want debug mode
    response = AskUserQuestion(
        questions=[
            {
                "question": "æ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼Ÿ\n\nå¯ç”¨åä¼šè®°å½•å®Œæ•´çš„å®¡æŸ¥è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n- æ‰€æœ‰å†³ç­–ç‚¹å’Œé€‰æ‹©\n- å­ä»£ç†å¯åŠ¨å’Œå®ŒæˆçŠ¶æ€\n- æ—¶é—´æˆ³å’Œè¿›åº¦ä¿¡æ¯\n- å®Œæ•´çš„äº¤äº’å†å²\n\nç”Ÿæˆçš„æ—¥å¿—å°†ä¿å­˜åˆ° DEBUG-SESSION.md æ–‡ä»¶ä¸­ã€‚",
                "header": "è°ƒè¯•æ¨¡å¼",
                "options": [
                    {
                        "label": "å¯ç”¨è°ƒè¯•",
                        "description": "è®°å½•å®Œæ•´å®¡æŸ¥è¿‡ç¨‹åˆ°DEBUG-SESSION.md"
                    },
                    {
                        "label": "ä¸å¯ç”¨",
                        "description": "ä»…æ˜¾ç¤ºåŸºæœ¬è¿›åº¦ä¿¡æ¯ï¼Œä¸ç”Ÿæˆè¯¦ç»†æ—¥å¿—"
                    }
                ],
                "multiSelect": False
            }
        ]
    )
    debug_mode = (response == "å¯ç”¨è°ƒè¯•")
```

**Debug Mode Behavior**:
- **Enabled**: Record all ğŸ” checkpoints, user choices, timestamps, subagent status to DEBUG-SESSION.md
- **Disabled**: Only show basic progress indicators, no detailed session log

**ğŸ” DEBUG Status**: `Debug mode = {debug_mode}`

---

### Step 1: Determine Review Scope

**ğŸ” DEBUG [Step 1/7]**: Starting - Determine Review Scope

Identify what code to review based on user input:

**Review Sources:**
- **Branch**: Single branch (e.g., `feature/auth`) - review all changes in branch
- **Branch Comparison**: Branch A vs Branch B (e.g., `feature/auth` vs `dev`) - **IMPORTANT**: Find merge base and diff from merge base to branch A's HEAD
- **MR/PR**: Merge Request (GitLab) or Pull Request (GitHub) by number or URL
- **Project**: Monorepo with multiple subprojects - ask which to review
- **Full Project**: Multiple independent projects or entire codebase - collect all project paths

**Required Information:**
- Branch names (if comparing branches)
- MR/PR number or URL
- Project paths (for monorepos or full project review)
- Repository URL (if not current directory)

**For Full Project Review:**
When user asks to "review entire project" or "review all code":
1. Ask user to specify which projects/directories to review
2. For each project, check if it's a git repository
3. Collect project metadata (tech stack, LOC, file count)
4. Confirm with user before proceeding

### Step 2: Establish Working Directory

**ğŸ” DEBUG [Step 2/7]**: Establishing working directory

**IMPORTANT**: Working directory name MUST include date and sequence number to avoid conflicts.

**Directory Naming Convention**: `{review_name}-{YYYYMMDD}-{sequence}`

**Generate unique working directory**:
```bash
# Get current date
DATE=$(date +%Y%m%d)

# Base directory name
BASE_DIR="{review_name}-${DATE}"

# Find existing directories with same base
EXISTING=$(ls -d reviews/${BASE_DIR}-* 2>/dev/null | wc -l)

# Calculate next sequence number
SEQUENCE=$((EXISTING + 1))

# Final directory name
WORKING_DIR="${BASE_DIR}-${SEQUENCE}"
```

**Examples**:
```
First review on 2026-01-30:    mr557-aihub-refactor-20260130-1
Second review on same day:     mr557-aihub-refactor-20260130-2
First review next day:         mr557-aihub-refactor-20260131-1
```

**Full path**: `{project_root}/reviews/{review_name}-{YYYYMMDD}-{sequence}`

**Implementation**:
```bash
# Example implementation
project_root="/home/user/myapp"
review_name="auth-feature"
date=$(date +%Y%m%d)

# Check for existing reviews today
existing_dirs=$(find "$project_root/reviews" -maxdepth 1 -name "${review_name}-${date}-*" | wc -l)
sequence=$((existing_dirs + 1))

working_dir="$project_root/reviews/${review_name}-${date}-${sequence}"
mkdir -p "$working_dir"
```

**Ask user for confirmation with generated directory name** (optional, can be auto-generated)

**Directory Structure:**
```
reviews/{review_name}-{YYYYMMDD}-{sequence}/
â”œâ”€â”€ code-context.json                     # All review metadata
â”œâ”€â”€ diff.patch                             # Git diff output
â”œâ”€â”€ commits.json                           # Commit history
â”œâ”€â”€ branch-info.json                       # Branch details
â”œâ”€â”€ DEBUG-SESSION.md                       # Debug session log (always uppercase)
â”œâ”€â”€ {review_name}-{YYYYMMDD}-{sequence}-comprehensive-summary.md # Final report
â””â”€â”€ reports/                               # Individual skill reports
    â”œâ”€â”€ skill1-report.md
    â”œâ”€â”€ skill2-report.md
    â””â”€â”€ ...
```

**IMPORTANT File Naming Conventions:**
1. **Working directory**: `{review_name}-{YYYYMMDD}-{sequence}` (date + sequence for uniqueness)
2. **Summary file**: `{review_name}-{YYYYMMDD}-{sequence}-comprehensive-summary.md` (include date+sequence)
3. **Debug session file**: `DEBUG-SESSION.md` (always uppercase, fixed name)
4. **Individual reports**: `{skill-name}-report.md` (use skill's short name)
5. **Context files**: lowercase with hyphens (code-context.json, diff.patch, etc.)

### Step 3: Collect and Save Code Content

**ğŸ” DEBUG [Step 3/7]**: Collecting code context and metadata

Collect comprehensive review information and save to working directory:

**Use `scripts/collect-review-data.sh`** to automate data collection.

**Save as `code-context.json`:**
```json
{
  "review_type": "branch_comparison|branch|mr|pr",
  "source_branch": "feature/auth",
  "target_branch": "dev",
  "merge_base": "abc123",
  "mr_number": "!1234",
  "pr_number": "567",
  "repository": "git@gitlab.com:group/project.git",
  "project_path": "/path/to/project",
  "working_directory": "/path/to/reviews/auth-feature-20260130-1",
  "review_date": "2026-01-30",
  "review_sequence": 1,
  "timestamp": "2026-01-30T14:30:22Z"
}
```

**Save as `diff.patch`:**
- Use `git diff merge_base...source_branch` for branch comparison
- Use `git diff dev...feature/auth` format (three dots) for correct merge base
- Include full context for review

**Save as `commits.json`:**
```json
{
  "commits": [
    {
      "hash": "def456",
      "author": "John Doe",
      "date": "2025-01-28T09:00:00Z",
      "message": "Add login form",
      "files_changed": ["src/auth/login.js"]
    }
  ]
}
```

**Save as `branch-info.json`:**
```json
{
  "source_branch": {
    "name": "feature/auth",
    "head_commit": "def456",
    "is_merged": false
  },
  "target_branch": {
    "name": "dev",
    "head_commit": "abc123"
  }
}
```

**For Full Project Review (non-Git or multi-project):**
```json
{
  "review_type": "full_project",
  "review_name": "full-project-review",
  "working_directory": "/path/to/reviews/full-project-review-20260130-1",
  "review_date": "2026-01-30",
  "review_sequence": 1,
  "projects": [
    {
      "name": "frontend",
      "path": "/path/to/frontend",
      "tech_stack": ["Nuxt.js", "Vue 2"],
      "language": "javascript"
    },
    {
      "name": "backend",
      "path": "/path/to/backend",
      "tech_stack": ["Spring Boot", "MyBatis"],
      "language": "java"
    }
  ]
}
```

**Critical for Branch Comparison:**
When comparing branch A vs branch B:
1. Find merge base: `git merge-base A B`
2. Diff from merge base to A: `git diff merge_base...A`
3. This ensures only unique changes in A are reviewed

**Confirm with User:**
After collecting code context, present to user using AskUserQuestion tool:

**ğŸ” DEBUG [Checkpoint 1]**: Display collected information and request confirmation

**IMPORTANT**: Use AskUserQuestion tool for user confirmation, not text prompts.

**Example AskUserQuestion call:**
```python
AskUserQuestion(
    questions=[
        {
            "question": "ä»£ç å®¡æŸ¥ä¿¡æ¯å·²æ”¶é›†ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ",
            "header": "ç¡®è®¤å®¡æŸ¥",
            "options": [
                {
                    "label": "ç»§ç»­å®¡æŸ¥",
                    "description": "å¼€å§‹æ‰§è¡Œä»£ç å®¡æŸ¥ï¼Œå¯åŠ¨å¹¶è¡Œå­ä»£ç†"
                },
                {
                    "label": "å–æ¶ˆ",
                    "description": "å–æ¶ˆæœ¬æ¬¡å®¡æŸ¥ï¼Œé€€å‡ºæŠ€èƒ½"
                }
            ],
            "multiSelect": false
        }
    ]
)
```

**Information to present in question description:**
```
Review Type: Full project review
Projects: 2 projects (frontend, backend)

Frontend:
  - Path: /projects/bupt/eduiot-lab
  - LOC: ~13,800
  - Tech Stack: Nuxt.js, Vue 2, Element UI

Backend:
  - Path: /projects/bupt/space-server
  - LOC: ~7,000
  - Tech Stack: Spring Boot, MyBatis, MySQL

Working Directory: /projects/bupt/reviews/full-project-review-20260130-1
```

**ğŸ” DEBUG**: Wait for user confirmation via AskUserQuestion before proceeding

**DO NOT proceed to Step 4 without user confirmation.**

### Step 4: Discover Available Review Skills (Multi-Round Selection)

**ğŸ” DEBUG [Step 4/7]**: Discovering available review skills with multi-round selection

**ğŸ” DEBUG**: Check system-reminder for available skills list

Identify which code review skills are available in the current environment.

**Check available skills:**
Look for skills with these patterns in their description:
- "code review", "review code", "review MR/PR"
- "security", "performance", "quality", "lint"

**Skill Category Mapping**:
Skills are organized into 4 functional categories:

```python
SKILL_CATEGORIES = {
    "ä»£ç è´¨é‡": [
        "code-review:code-review",
        "comprehensive-review:code-reviewer",
        "code-review-ai:code-review",
        "codebase-cleanup:code-reviewer",
        "feature-dev:code-reviewer",
        "code-documentation:code-reviewer"
    ],
    "å®‰å…¨å®¡è®¡": [
        "security-scanning:security-auditor",
        "comprehensive-review:security-auditor",
        "security-scanning:threat-modeling-expert"
    ],
    "æ€§èƒ½+æ¶æ„": [
        "comprehensive-review:architect-review",
        "application-performance:performance-engineer",
        "backend-development:backend-architect",
        "application-performance:observability-engineer"
    ],
    "æµ‹è¯•+æ¸…ç†": [
        "pr-review-toolkit:pr-test-analyzer",
        "unit-testing:test-automator",
        "pr-review-toolkit:code-simplifier",
        "pr-review-toolkit:comment-analyzer",
        "pr-review-toolkit:type-design-analyzer"
    ]
}
```

**Skill Discovery Process:**
1. Review the list of available skills in system-reminder
2. Organize skills into the 4 categories above
3. Present findings to user in DEBUG output

---

### Step 4.1: Display All Skills in DEBUG Output

**ğŸ” DEBUG [Checkpoint 2.1]**: Display all discovered skills by category

**CRITICAL**: Always display ALL discovered skills in DEBUG output before selection.

**Example DEBUG output:**
```python
print("ğŸ” å‘ç°å¯ç”¨çš„å®¡æŸ¥æŠ€èƒ½\n")
print("=" * 70)

print("\n**ä»£ç è´¨é‡** (6ä¸ªæŠ€èƒ½):")
for skill in SKILL_CATEGORIES["ä»£ç è´¨é‡"]:
    print(f"  â€¢ {skill}")

print("\n**å®‰å…¨å®¡è®¡** (3ä¸ªæŠ€èƒ½):")
for skill in SKILL_CATEGORIES["å®‰å…¨å®¡è®¡"]:
    print(f"  â€¢ {skill}")

print("\n**æ€§èƒ½+æ¶æ„** (4ä¸ªæŠ€èƒ½):")
for skill in SKILL_CATEGORIES["æ€§èƒ½+æ¶æ„"]:
    print(f"  â€¢ {skill}")

print("\n**æµ‹è¯•+æ¸…ç†** (5ä¸ªæŠ€èƒ½):")
for skill in SKILL_CATEGORIES["æµ‹è¯•+æ¸…ç†"]:
    print(f"  â€¢ {skill}")

print("=" * 70)
print(f"ğŸ” å…±å‘ç° {sum(len(v) for v in SKILL_CATEGORIES.values())} ä¸ªå®¡æŸ¥æŠ€èƒ½\n")
```

---

### Step 4.2: Round 1 - Select Review Categories

**ğŸ” DEBUG [Checkpoint 2.2]**: First round selection - choose categories

**IMPORTANT**: Use AskUserQuestion tool for category selection.

**AskUserQuestion call:**
```python
AskUserQuestion(
    questions=[
        {
            "question": f"""
è¯·é€‰æ‹©å®¡æŸ¥ç±»åˆ«ï¼ˆå¯å¤šé€‰ï¼‰:

**ä»£ç è´¨é‡** (6ä¸ªæŠ€èƒ½): ä»£ç è§„èŒƒã€æ½œåœ¨bugã€å¯ç»´æŠ¤æ€§ã€æ¶æ„åˆ†æã€ä»£ç æ¸…ç†
**å®‰å…¨å®¡è®¡** (3ä¸ªæŠ€èƒ½): å®‰å…¨æ¼æ´ã€OWASP Top 10ã€å¨èƒå»ºæ¨¡
**æ€§èƒ½+æ¶æ„** (4ä¸ªæŠ€èƒ½): æ€§èƒ½ä¼˜åŒ–ã€æ¶æ„å®¡æŸ¥ã€è®¾è®¡æ¨¡å¼ã€åç«¯æ¶æ„
**æµ‹è¯•+æ¸…ç†** (5ä¸ªæŠ€èƒ½): æµ‹è¯•è¦†ç›–ã€ä»£ç æ¸…ç†ã€ä»£ç ç®€åŒ–ã€æ³¨é‡Šåˆ†æ

**å¾…å®¡æŸ¥é¡¹ç›®**:
- å‰ç«¯: Nuxt.js + Vue 2 (~118 æ–‡ä»¶)
- åç«¯: Spring Boot + Java 21 (~107 Javaæ–‡ä»¶)

**æç¤º**: é€‰æ‹©å¤šä¸ªç±»åˆ«å¯ä»¥è¿›è¡Œæ›´å…¨é¢çš„å®¡æŸ¥
""",
            "header": "å®¡æŸ¥ç±»åˆ«",
            "options": [
                {
                    "label": "ä»£ç è´¨é‡",
                    "description": "åŒ…å« code-review, comprehensive-reviewer, code-review-ai, codebase-cleanup, feature-dev, code-documentation"
                },
                {
                    "label": "å®‰å…¨å®¡è®¡",
                    "description": "åŒ…å« security-auditor, comprehensive-security, threat-modeling-expert"
                },
                {
                    "label": "æ€§èƒ½+æ¶æ„",
                    "description": "åŒ…å« architect-review, performance-engineer, backend-architect, observability-engineer"
                },
                {
                    "label": "æµ‹è¯•+æ¸…ç†",
                    "description": "åŒ…å« pr-test-analyzer, test-automator, code-simplifier, comment-analyzer, type-design-analyzer"
                }
            ],
            "multiSelect": True
        }
    ]
)
```

**ğŸ” DEBUG**: Show user's category selection: `["ä»£ç è´¨é‡", "å®‰å…¨å®¡è®¡"]`

---

### Step 4.3: Round 2 - Select Specific Skills

**ğŸ” DEBUG [Checkpoint 2.3]**: Second round selection - choose specific skills

For each category selected in Round 1, present specific skills.

**Example: User selected "ä»£ç è´¨é‡" category**

```python
AskUserQuestion(
    questions=[
        {
            "question": """
è¯·é€‰æ‹©**ä»£ç è´¨é‡**ç±»åˆ«çš„å…·ä½“æŠ€èƒ½ï¼ˆå¯å¤šé€‰ï¼‰:

**é€šç”¨å®¡æŸ¥**: code-review:code-review - ä»£ç è§„èŒƒã€bugã€å¯ç»´æŠ¤æ€§
**æ·±åº¦åˆ†æ**: comprehensive-review:code-reviewer - æ¶æ„ã€è®¾è®¡æ¨¡å¼
**AIé©±åŠ¨**: code-review-ai:code-review - AIå¢å¼ºçš„ä»£ç å®¡æŸ¥
**ä»£ç æ¸…ç†**: codebase-cleanup:code-reviewer - ä¼˜åŒ–ã€ç®€åŒ–
**åŠŸèƒ½å¼€å‘**: feature-dev:code-reviewer - åŠŸèƒ½å¼€å‘å®¡æŸ¥
**æ–‡æ¡£å®¡æŸ¥**: code-documentation:code-reviewer - ç²¾è‹±ä»£ç å®¡æŸ¥
""",
            "header": "ä»£ç è´¨é‡æŠ€èƒ½",
            "options": [
                {
                    "label": "code-review:code-review",
                    "description": "é€šç”¨ä»£ç è´¨é‡ - ä»£ç è§„èŒƒã€bugã€å¯ç»´æŠ¤æ€§"
                },
                {
                    "label": "comprehensive-review:code-reviewer",
                    "description": "æ·±åº¦ä»£ç åˆ†æ - æ¶æ„ã€è®¾è®¡æ¨¡å¼"
                },
                {
                    "label": "codebase-cleanup:code-reviewer",
                    "description": "ä»£ç æ¸…ç† - ä¼˜åŒ–ã€ç®€åŒ–"
                },
                {
                    "label": "ä½¿ç”¨å…¨éƒ¨ä»£ç è´¨é‡æŠ€èƒ½",
                    "description": "ä½¿ç”¨è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰6ä¸ªæŠ€èƒ½"
                }
            ],
            "multiSelect": True
        }
    ]
)
```

**Example: User selected "å®‰å…¨å®¡è®¡" category**

```python
AskUserQuestion(
    questions=[
        {
            "question": """
è¯·é€‰æ‹©**å®‰å…¨å®¡è®¡**ç±»åˆ«çš„å…·ä½“æŠ€èƒ½ï¼ˆå¯å¤šé€‰ï¼‰:

**å®‰å…¨æ¼æ´**: security-scanning:security-auditor - OWASP Top 10ã€æ³¨å…¥æ”»å‡»
**ç»¼åˆå®‰å…¨**: comprehensive-review:security-auditor - å…¨é¢å®‰å…¨åˆ†æ
**å¨èƒå»ºæ¨¡**: security-scanning:threat-modeling-expert - å®‰å…¨æ¶æ„åˆ†æ
""",
            "header": "å®‰å…¨å®¡è®¡æŠ€èƒ½",
            "options": [
                {
                    "label": "security-scanning:security-auditor",
                    "description": "å®‰å…¨æ¼æ´ - OWASP Top 10ã€æ³¨å…¥æ”»å‡»"
                },
                {
                    "label": "comprehensive-review:security-auditor",
                    "description": "ç»¼åˆå®‰å…¨å®¡è®¡ - å…¨é¢å®‰å…¨åˆ†æ"
                },
                {
                    "label": "security-scanning:threat-modeling-expert",
                    "description": "å¨èƒå»ºæ¨¡ - å®‰å…¨æ¶æ„åˆ†æ"
                },
                {
                    "label": "ä½¿ç”¨å…¨éƒ¨å®‰å…¨å®¡è®¡æŠ€èƒ½",
                    "description": "ä½¿ç”¨è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰3ä¸ªæŠ€èƒ½"
                }
            ],
            "multiSelect": True
        }
    ]
)
```

**CRITICAL Rules for Skill Selection:**
1. **Multi-round selection**: First select categories, then select specific skills
2. **Always display ALL skills in DEBUG output** before selection
3. **AskUserQuestion label MUST be exact skill name** (e.g., "code-review:code-review")
4. **Use multiSelect: True** for both rounds
5. **Offer "ä½¿ç”¨å…¨éƒ¨[ç±»åˆ«]æŠ€èƒ½" option** for convenience
6. **Include project details** to help user choose appropriate skills

**ğŸ” DEBUG**: Show final skill selection: `["code-review:code-review", "security-scanning:security-auditor", ...]`

**Ask user to select which skills to use using AskUserQuestion.**
**DO NOT proceed to Step 5 without user skill selection.**

### Step 5: Launch Parallel Subagents

**ğŸ” DEBUG [Step 5/7]**: Launching parallel subagents with review skills

**ğŸ” DEBUG**: Show selected skills and subagent configuration before launch

**Use Task tool with run_in_background=true** to launch multiple subagents in parallel.

**CRITICAL**: Each subagent MUST use a DIFFERENT review skill via the Skill tool.

**Example parallel launch:**

**ğŸ” DEBUG [Checkpoint 3]**: Display subagent launch configuration

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ Launching Parallel Subagents
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Subagent 1: code-review:code-review
  - Review scope: Frontend (Nuxt.js)
  - Output: reports/code-review-report.md

Subagent 2: security-scanning:security-auditor
  - Review scope: Both projects
  - Output: reports/security-report.md

Subagent 3: pr-review-toolkit:review-pr
  - Review scope: All files
  - Output: reports/pr-review-report.md
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**ğŸ” DEBUG**: Track subagent status
```
Agent 1 (code-review): â³ Starting...
Agent 2 (security):    â³ Starting...
Agent 3 (pr-review):    â³ Starting...
```

**Provide each subagent with:**
- Location of `code-context.json`
- Location of `diff.patch` (for git reviews) OR project paths (for full project review)
- Output report path: `reports/{skill-name}-report.md`
- **INSTRUCTION to use Skill tool to invoke the review skill**

**Subagent Prompt Template:**
```markdown
You are reviewing code as part of a comprehensive code review.

**Your assigned skill**: {skill_name}

**Task**:
1. Use the Skill tool to invoke: {skill_name}
2. Provide the skill with:
   - Review scope: {scope_description}
   - Code location: {code_path}
   - Any additional context from code-context.json
3. Generate a comprehensive report following that skill's workflow
4. Save your report to: {output_path}

**IMPORTANT**:
- You MUST use the Skill tool to invoke {skill_name}
- Do NOT review code manually - let the skill guide you
- The skill will provide the specific review methodology
- Follow the skill's workflow exactly
```

**Example Task tool calls:**
```yaml
Task 1:
  subagent_type: general-purpose
  description: Review using code-review:code-review
  run_in_background: true
  prompt: |
    You are reviewing the frontend code using the code-review:code-review skill.
    Project path: /projects/bupt/eduiot-lab
    Output: /projects/bupt/reviews/full-project-review/reports/code-review-report.md
    Use the Skill tool to invoke code-review:code-review

Task 2:
  subagent_type: general-purpose
  description: Review using security-scanning:security-auditor
  run_in_background: true
  prompt: |
    You are reviewing both frontend and backend for security issues.
    Frontend: /projects/bupt/eduiot-lab
    Backend: /projects/bupt/space-server
    Output: /projects/bupt/reviews/full-project-review/reports/security-report.md
    Use the Skill tool to invoke security-scanning:security-auditor

Task 3:
  subagent_type: general-purpose
  description: Review using pr-review-toolkit:review-pr
  run_in_background: true
  prompt: |
    You are reviewing code quality using pr-review-toolkit:review-pr skill.
    Review all files in both projects.
    Output: /projects/bupt/reviews/full-project-review/reports/pr-review-report.md
    Use the Skill tool to invoke pr-review-toolkit:review-pr
```

**File Writing Strategy:**
- Subagents should use Write tool to save their reports
- If subagents cannot write files, they should output the full report content
- Main agent: Collect all outputs and save using Write tool
- Ensure `reports/` directory exists before launching subagents

**Wait for all subagents to complete** using TaskOutput tool before proceeding to Step 6.

**ğŸ” DEBUG**: Show subagent completion status
```
Agent 1 (code-review): âœ… Complete
Agent 2 (security):    âœ… Complete
Agent 3 (pr-review):    âœ… Complete

All reports generated successfully!
```

### Step 6: Generate Consolidated Summary

**ğŸ” DEBUG [Step 6/7]**: Generating consolidated summary from all reports

**ğŸ” DEBUG [Checkpoint 4]**: Display report collection status

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Collecting Reports from Subagents
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Found 3 reports in reports/ directory:
âœ“ code-review-report.md (32 issues found)
âœ“ security-report.md (19 issues found)
âœ“ pr-review-report.md (25 issues found)

Total issues to consolidate: 76 issues
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**ğŸ” DEBUG**: Show categorization progress
```
Categorizing issues by severity...
- Critical: 3 issues
- High: 13 issues
- Medium: 31 issues
- Low: 29 issues
```

**Read all individual reports** from `reports/` directory.

**Analyze findings and categorize by severity:**
- **Critical**: Security vulnerabilities, crashes, data loss risks
- **High**: Major bugs, performance issues, breaking changes
- **Medium**: Code smells, maintainability issues
- **Low**: Style issues, minor optimizations

**Create `{review_name}-{YYYYMMDD}-{sequence}-comprehensive-summary.md`:**

**IMPORTANT File Naming Convention:**
- **Summary file**: `{review_name}-{YYYYMMDD}-{sequence}-comprehensive-summary.md` (include date+sequence)
- **Debug session file**: `DEBUG-SESSION.md` (always uppercase, fixed name)

**Structure:**
```markdown
# Code Review Comprehensive Summary: {review_name}

## ğŸ¤– Review Skills Used

This review used multiple AI skills, each analyzing from different perspectives:

| Skill Name | Focus Area | Key Contributions |
|------------|------------|-------------------|
| code-review:code-review | ä»£ç è´¨é‡ä¸æœ€ä½³å®è·µ | ä»£ç è§„èŒƒã€æ½œåœ¨bugã€å¯ç»´æŠ¤æ€§ |
| security-scanning:security-auditor | å®‰å…¨æ¼æ´å®¡è®¡ | OWASP Top 10ã€æ³¨å…¥æ”»å‡»ã€è®¤è¯æˆæƒ |
| pr-review-toolkit:review-pr | å…¨é¢PRå®¡æŸ¥ | åŠŸèƒ½å®Œæ•´æ€§ã€æµ‹è¯•è¦†ç›–ã€æ–‡æ¡£ |

**Total Issues Found**: X issues (after deduplication)

## Overview
- Review Type: Branch comparison (feature/auth vs dev)
- Commits: 5 commits
- Files changed: 12 files
- Review Skills: 3 skills used in parallel
- Date: 2025-01-28

## Findings Summary
- Critical: 2 issues
- High: 5 issues
- Medium: 8 issues
- Low: 3 issues

## ğŸ”´ Critical Issues

### 1. SQL Injection Risk in auth/login.js
- **Location**: `src/auth/login.js:45`
- **Severity**: Critical
- **Found by**: code-review:code-review, security-scanning:security-auditor
- **Issue**: User input directly concatenated into SQL query
- **Recommendation**: Use parameterized queries
- **Code snippet**:
  ```javascript
  // Current (unsafe)
  const query = `SELECT * FROM users WHERE name = '${username}'`

  // Suggested (safe)
  const query = 'SELECT * FROM users WHERE name = ?'
  db.query(query, [username])
  ```

### 2. Authentication Bypass
- **Location**: `src/auth/check.js:12`
- **Severity**: Critical
- **Found by**: security-scanning:security-auditor, pr-review-toolkit:review-pr
- **Issue**: Missing authentication check on admin endpoint
- **Recommendation**: Add authentication middleware

## ğŸŸ  High Priority Issues

### 1. Missing Error Handling in API client
- **Location**: `src/api/client.js:78`
- **Severity**: High
- **Found by**: code-review:code-review
- **Issue**: No try-catch around fetch request
- **Recommendation**: Add error handling with retry logic

## ğŸŸ¡ Medium Priority Issues

### 1. Inconsistent Naming Convention
- **Location**: Multiple files
- **Severity**: Medium
- **Found by**: code-review:code-review
- **Issue**: Mix of camelCase and snake_case
- **Recommendation**: Standardize on camelCase

## ğŸŸ¢ Low Priority Issues

### 1. Unused Imports
- **Location**: `src/utils/helpers.js:3`
- **Severity**: Low
- **Found by**: code-review:code-review
- **Issue**: Import 'lodash' unused
- **Recommendation**: Remove unused imports

**CRITICAL TEMPLATE RULES**:
1. **EVERY issue MUST include "Found by" field**
2. Use **complete skill names** (e.g., "code-review:code-review, security-scanning:security-auditor")
3. Use **comma separation** for multiple skills
4. **DO NOT use abbreviations** or symbols (not "[CR]", "[SA]", etc.)
5. If unsure which skill found it, check the individual skill reports
6. If multiple skills found same issue, list ALL of them in "Found by"

## ğŸ“Š Skill Contributions Summary

### code-review:code-review
**Issues Found**: X
**Focus**: ä»£ç è´¨é‡ä¸æœ€ä½³å®è·µ
**Key Findings**:
- Finding 1
- Finding 2

### security-scanning:security-auditor
**Issues Found**: Y
**Focus**: å®‰å…¨æ¼æ´å®¡è®¡
**Key Findings**:
- Finding 1
- Finding 2

### pr-review-toolkit:review-pr
**Issues Found**: Z
**Focus**: å…¨é¢PRå®¡æŸ¥
**Key Findings**:
- Finding 1
- Finding 2

## Detailed Reports

Individual skill reports:
- [code-review:code-review report](reports/code-review-report.md)
- [security-scanning:security-auditor report](reports/security-auditor-report.md)
- [pr-review-toolkit:review-pr report](reports/pr-review-report.md)
```

### Step 7: Generate Debug Session Log (If Debug Mode Enabled)

**ğŸ” DEBUG [Step 7/7]**: Generating DEBUG-SESSION.md

**CRITICAL**: Only execute this step if `debug_mode = True` from Step 0.

**When to Generate**: After comprehensive summary is complete, before interactive issue resolution.

**What to Record**:
```markdown
# Code Review Orchestrator - Session Debug Log

**Session Date**: {CURRENT_DATE}
**Session ID**: {review_name}-{YYYYMMDD}-{sequence}
**Working Directory**: `{full_path_to_working_dir}`
**Skill**: code-review-orchestrator
**Status**: âœ… COMPLETED

---

## 1. Session Configuration

### User Input
- **åŸå§‹è¯·æ±‚**: "{user's original request}"
- **æ„å›¾**: {review_type}
- **é¡¹ç›®æ•°é‡**: {number_of_projects}

### Identified Projects
**Project 1 ({name})**:
- Path: `{path}`
- Tech Stack: {tech_stack}
- File Count: {count}
- Language: {language}

### Working Directory Creation
```bash
DATE={date}
EXISTING_COUNT={existing_count}
SEQUENCE={sequence}
WORKING_DIR="{working_dir}"
# Result: {final_working_dir}
```

---

## 2. Execution Timeline

### Step 0: Debug Mode Detection
**Time**: {start_time} - {end_time}
- âœ… Debug mode: {ENABLED/DISABLED}
- Detection method: {keyword_detected / user_confirmed}

### Step 1: Determine Review Scope
**Time**: {start_time} - {end_time}
- âœ… Identified as: {review_type}
- âœ… Discovered {number} projects
- âœ… Collected project metadata

### Step 2: Establish Working Directory
**Time**: {start_time}
- âœ… Created unique directory with date and sequence
- âœ… Created reports/ subdirectory
- âœ… Confirmed directory structure

### Step 3: Collect Code Context
**Time**: {start_time} - {end_time}
- âœ… Saved code-context.json
- **Content**: Project metadata, tech stack, file counts

### Step 4: User Confirmation
**Time**: {time}
- âœ… Used AskUserQuestion tool
- **User Choice**: "{choice}"
- Confirmation received

### Step 5: Skill Selection
**Time**: {start_time} - {end_time}
- âœ… Discovered available skills
- âœ… Presented skill options to user
- **User Choice**: "{choice}"

**Selected Skills**:
1. {skill_name_1}
2. {skill_name_2}
3. {skill_name_3}
...

### Step 6: Launch Subagents
**Time**: {start_time} - {end_time}

**Subagent 1**:
- Agent ID: {agent_id}
- Task: {task_description}
- Status: âœ… Completed
- Output: {output_path}
- Report: {report_path}

**Subagent 2**:
- Agent ID: {agent_id}
- Task: {task_description}
- Status: âœ… Completed
- Output: {output_path}
- Report: {report_path}

...

### Step 7: Generate Comprehensive Summary
**Time**: {start_time} - {end_time}
- âœ… Read all generated reports
- âœ… Consolidated findings
- âœ… Created comprehensive summary
- âœ… Saved: {summary_file_name}

**Total Duration**: ~{total_time}

---

## 3. Files Generated

### Primary Output Files
```
{working_directory}/
â”œâ”€â”€ code-context.json                              # Review metadata
â”œâ”€â”€ {review_name}-{YYYYMMDD}-{sequence}-comprehensive-summary.md  # Main report
â””â”€â”€ reports/
    â”œâ”€â”€ skill1-report.md                           # {size}, {lines} lines
    â”œâ”€â”€ skill2-report.md                           # {size}, {lines} lines
    â””â”€â”€ ...
```

---

## 4. Key Decisions and Rationale

### Decision 1: Review Type Determination
**Input**: "{user_input}"
**Analysis**: {analysis}
**Decision**: {decision}
**Rationale**: {rationale}

### Decision 2: Skill Selection Strategy
**Options Presented**: {options}
**User Choice**: "{user_choice}"
**Selected Skills**: {skills_list}
**Rationale**: {rationale}

---

## 5. Issues Encountered

### Issue 1: {issue_title}
**Problem**: {problem_description}
**Resolution**: {resolution}
**Impact**: {impact}

---

## 6. Performance Metrics

### Execution Time
- Total Duration: ~{total_time}
- Per Subagent Average: ~{average_time}
- Consolidation: ~{consolidation_time}

### Resource Usage
- Subagents: {number} parallel agents
- Tokens Used: Estimated {token_count}+ tokens
- Files Generated: {number} files (~{total_lines} lines total)

### Success Rate
- Subagents Completed: {completed}/{total} ({percentage}%)
- Reports Saved: {saved}/{total} ({percentage}%)
- Findings Integrated: {integrated}/{total} ({percentage}%)

---

## 7. Findings Summary

### By Dimension
**Code Quality**: {grade}
**Security**: {grade}
**Architecture**: {grade}
**Test Coverage**: {grade}

---

## 8. Recommendations for Improvement

### Process Improvements
1. {improvement_1}
2. {improvement_2}
3. {improvement_3}

---

## 9. Session Context

### Initial Request
```
User: {user_command}
Arguments: {user_arguments}
```

### Execution Flow
1. {step_1}
2. {step_2}
3. {step_3}
...

---

## 10. Technical Details

### Environment
- **Platform**: {platform}
- **OS**: {os_version}
- **Date**: {date}
- **Working Directory**: {working_dir}
- **Git Repository**: {git_status}

### Tool Versions
- **Claude Code**: {version}
- **Skill Version**: code-review-orchestrator v{version}
- **Subagents**: {agent_type} ({number} instances)

---

## 11. Error Log

### Errors Encountered
{if_errors_exist}
### Error 1: {error_title}
```
{error_details}
```
**Resolution**: {resolution}
{else}
No errors encountered during this session.
{end_if}

---

**END OF SESSION DEBUG LOG**

Generated: {generation_time}
Logger: Claude Code (code-review-orchestrator skill v{version})
Session: {session_id}
```

**Save Location**: `{working_directory}/DEBUG-SESSION.md`

**ğŸ” DEBUG**: Session log saved to DEBUG-SESSION.md

---

### Step 8: Interactive Issue Resolution

**After generating summary, present actionable next steps:**

```
Found 18 issues. Which issues would you like to fix?

Options:
1. Fix all Critical issues (2)
2. Fix all High priority issues (5)
3. Fix specific issues (select by number)
4. Review specific issues first
5. Skip fixing for now

Enter your choice:
```

**If user chooses to fix issues:**
- Use appropriate development skills (e.g., `feature-dev:feature-dev`)
- Create implementation plan for fixes
- Apply fixes with user confirmation
- Verify fixes don't introduce new issues

## Additional Resources

### Scripts

- **`scripts/collect-review-data.sh`** - Automates collection of diff, commits, branch info
- **`scripts/find-merge-base.sh`** - Finds merge base for branch comparison
- **`scripts/launch-subagents.sh`** - Launches parallel review subagents

### References

- **`references/subagent-coordination.md`** - Detailed guide on coordinating multiple subagents
- **`references/report-formatting.md`** - Report structure and formatting standards
- **`references/issue-categories.md`** - Issue classification and severity guidelines

### Examples

- **`examples/review-session-output/`** - Complete example of a review session
- **`examples/code-context-example.json`** - Sample code context file
- **`examples/summary-example.md`** - Sample consolidated summary

## Best Practices

### Branch Comparison

**Always use three-dot diff** (`git diff A...B`) for branch comparison:
- `git diff dev...feature/auth` - Changes since branches diverged
- NOT `git diff dev feature/auth` - Changes between branch heads (wrong)

**Example:**
```bash
# Find merge base
MERGE_BASE=$(git merge-base dev feature/auth)

# Diff from merge base to feature branch
git diff $MERGE_BASE...feature/auth > diff.patch
```

### Full Project Review

When reviewing entire projects or multiple independent projects:

**1. Discover Project Structure**
- Use `ls` and `find` to understand directory layout
- Check for package.json, pom.xml, requirements.txt, etc.
- Identify tech stack and language
- Count lines of code

**2. Collect Project Metadata**
```bash
# Example: Frontend project
cd /projects/bupt/eduiot-lab
find . -name "*.vue" -o -name "*.js" | wc -l  # Count files
cat package.json  # Identify framework

# Example: Backend project
cd /projects/bupt/space-server
find . -name "*.java" | wc -l  # Count files
cat pom.xml  # Identify framework
```

**3. Use Appropriate Review Skills**
- For frontend: code-review:code-review, javascript-typescript:javascript-pro
- For backend: code-review:code-review, jvm-languages:java-pro
- For security: security-scanning:security-auditor
- For architecture: code-review-ai:architect-review

**4. Coordinate Subagent Communication**
- Each subagent reviews independently
- No inter-subagent communication needed
- Main agent consolidates all reports
- Use file system for data sharing

### Parallel Subagent Execution

**Launch subagents in parallel** using Task tool with `run_in_background=true`:
```yaml
subagent_type: general-purpose
run_in_background: true
prompt: |
  Review the code in /path/to/diff.patch
  Use the code-review:code-review skill
  Output report to /path/to/reports/code-review-report.md
```

**Wait for completion** using TaskOutput tool before generating summary.

### Report Consolidation

**Read all reports** before generating summary:
- Use Read tool to load each report
- Extract findings with severity levels
- Cross-reference duplicate findings
- Prioritize by severity

**Categorize issues** using severity guidelines in `references/issue-categories.md`.

### User Interaction

**Ask before making changes**:
- Present issues in prioritized list
- Let user choose which to fix
- Confirm each fix before applying
- Provide rollback options

## Troubleshooting

### Project Not Recognized

**Problem**: Commands like `cd space-server` fail with "No such file or directory"

**Root Cause**: Skill assumes git workflow, but user has independent projects

**Solutions:**
1. Identify this is "full project review", not branch comparison
2. Use absolute paths to projects
3. Don't use `cd` - use full paths in commands
4. Ask user to confirm project paths

**Example:**
```bash
# WRONG
cd space-server && git log

# RIGHT
cd /projects/bupt/space-server && git log
# OR
git -C /projects/bupt/space-server log
```

### No Diff Output

**Problem**: Empty `diff.patch` file

**Solutions:**
- Verify branch names are correct
- Check merge base calculation
- Ensure branches have diverged
- Use `git log --oneline A..B` to verify commits exist

### Subagent Failures

**Problem**: Subagent crashes or times out

**Solutions:**
- Check subagent logs for errors
- Verify skill is available
- Reduce scope (fewer files)
- Increase timeout limits

### Subagents Cannot Write Files

**Problem**: Report files not created in `reports/` directory

**Root Cause**: Subagents may not have write permissions or Write tool access

**Solutions:**
1. Main agent creates `reports/` directory before launching subagents
2. Subagent attempts to write file using Write tool
3. If write fails, subagent outputs full report content as text
4. Main agent collects outputs and saves using Write tool

**Pattern:**
```yaml
# Main agent
mkdir -p reports/

# Launch subagent with fallback instruction
Task(prompt: |
  1. Perform review using {skill}
  2. Try to save report to: reports/{skill}-report.md
  3. If Write tool fails, output full report as markdown text
  4. Include "REPORT_START" and "REPORT_END" markers
)

# Main agent collects output
TaskOutput(task_id, block=true)
Read output file, extract report between markers
Save using Write tool
```

### Skills Not Discovered

**Problem**: No review skills found or presented to user

**Solutions:**
- Check system-reminder for available skills list
- Look for skills with "review" in description
- Ask user which skills they want to use
- Fall back to general-purpose agents with custom prompts

### Duplicate Findings

**Problem**: Multiple skills report same issue

**Solutions:**
- Group by file and line number
- Cite all skills that found it
- Consolidate into single finding
- Note which found it first

## Technical Notes

### Git Diff Formats

- **Two dots (`git diff A..B`)**: Diff between A and B tips
- **Three dots (`git diff A...B`)**: Diff from merge base to B (correct for review)
- **Use three dots** for branch comparison reviews

### Subagent Communication

- Each subagent works independently
- No inter-subagent communication needed
- Consolidation happens after all complete
- Use file system for data sharing

### Performance Considerations

- Large diffs (>10,000 lines): Consider splitting
- Many files (>100): Review in batches
- Many subagents (>5): Limit parallelism
- Cache results for repeated reviews
