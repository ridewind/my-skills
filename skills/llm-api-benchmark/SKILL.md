---
name: llm-api-benchmark
description: This skill should be used when the user wants to "test API speed", "benchmark LLM", "check API latency", "measure response time", "test TPS", "benchmark endpoint performance", or wants to evaluate multiple LLM API endpoints' performance metrics.
---

# LLM API Benchmark

Automatically detects the current LLM API endpoint from environment variables and performs performance benchmarking.

## Default Behavior

When invoked directly without any parameters (e.g., `/llm-api-benchmark` or "Run benchmark"), the skill executes with these defaults:

- **Preset**: `code` (programming-related prompt, ~500-1000 tokens output)
- **Iterations**: `5`
- **Output**: Reports saved to `reports/llm-benchmark-{timestamp}/`

> The default uses the `code` preset optimized for coding workflows, generating substantial output for accurate TPS measurement.

## Usage

Simply invoke this skill when you want to benchmark your current LLM API:

```
"Test API speed"
"Run benchmark"
"测试API速度"
"测试响应时间"
```

Or use the command directly:

```bash
python skills/llm-api-benchmark/scripts/benchmark.py
```

## Supported Providers

The tool auto-detects these providers from environment variables:

| Environment Variable | Provider |
|---------------------|----------|
| `ANTHROPIC_API_KEY` | Anthropic/Claude |
| `OPENAI_API_KEY` | OpenAI |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI |
| `GOOGLE_GENERATIVE_AI_API_KEY` | Google Gemini |
| `AWS_ACCESS_KEY_ID` | AWS Bedrock |

## Options

```bash
# List available presets
python benchmark.py --list-presets

# Use preset (recommended for consistent results)
python benchmark.py --preset throughput   # For TPS testing (~300-500 tokens)
python benchmark.py --preset quick        # Fast test (~10 tokens)
python benchmark.py --preset standard     # Medium (~20 tokens)

# Custom iterations
python benchmark.py --iterations 10

# Custom model
python benchmark.py --model gpt-4o

# Custom prompt
python benchmark.py --prompt "Your test prompt"

# Custom output directory
python benchmark.py --output-dir ./my-reports

# Quiet mode (less output)
python benchmark.py --preset throughput -q
```

## Presets

| Preset | Description | Expected Output |
|--------|-------------|-----------------|
| `quick` | Short prompt for fast testing | ~10 tokens |
| `standard` | Medium-length prompt | ~20 tokens |
| `long` | Longer output test | ~100+ tokens |
| `throughput` | High token output for TPS testing | ~300-500 tokens |
| `code` | Programming-related prompt (default) | ~500-1000 tokens |
| `json` | Structured JSON output test | ~30 tokens |

**Recommended**: The `code` preset (default) or `throughput` preset for accurate TPS measurement in coding workflows.

## Output

Reports are saved to `reports/llm-benchmark-{timestamp}/`:

```
reports/llm-benchmark-20260227-103000/
├── benchmark-report.md    # Human-readable report
└── benchmark-data.json    # Raw data for programmatic access
```

## Metrics

The benchmark measures:

- **Response Time**: End-to-end latency
- **TTFT** (Time To First Token): Time until first token received
- **TPS** (Tokens Per Second): Token generation throughput

Statistics include: average, min, max, P50, P95, P99

## Example Report

Below is a sample benchmark report generated by the tool:

```markdown
# LLM API Benchmark Report

## Test Information
- **Time**: 2026-02-27T10:30:00.123456
- **Provider**: Anthropic
- **Endpoint**: https://api.anthropic.com/v1/messages
- **Model**: claude-sonnet-4-20250514
- **Prompt**: Count from 1 to 10, one number per line. Just output the numbers.
- **Iterations**: 5

## Performance Metrics

### Response Time (seconds)
| Metric | Value |
|--------|-------|
| Average | 0.847s |
| Minimum | 0.723s |
| Maximum | 1.012s |
| P50 | 0.835s |
| P95 | 1.012s |
| P99 | 1.012s |

### Time to First Token (TTFT)
| Metric | Value |
|--------|-------|
| Average | 0.342s |
| Minimum | 0.289s |
| Maximum | 0.401s |

### Tokens Per Second (TPS)
| Metric | Value |
|--------|-------|
| Average | 28.45 |
| Minimum | 24.12 |
| Maximum | 32.67 |

**Total Tokens**: 120

## Detailed Results

| # | Response Time | TTFT | Tokens | TPS | Status |
|---|---------------|------|--------|-----|--------|
| 1 | 0.823s | 0.312s | 24 | 29.12 | OK |
| 2 | 0.723s | 0.289s | 24 | 32.67 | OK |
| 3 | 1.012s | 0.401s | 24 | 24.12 | OK |
| 4 | 0.835s | 0.345s | 24 | 28.74 | OK |
| 5 | 0.842s | 0.362s | 24 | 27.61 | OK |
```

**Interpreting the metrics:**
- **Response Time**: Total time from request to completion
- **TTFT** (Time To First Token): How fast the first token appears (indicates processing start latency)
- **TPS** (Tokens Per Second): Generation throughput after first token

Lower response time and TTFT are better. Higher TPS is better.